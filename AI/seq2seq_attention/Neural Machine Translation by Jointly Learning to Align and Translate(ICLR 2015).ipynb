{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DZeY69nKuES"
   },
   "source": [
    "# Neural Machine Translation by Jointly Learning to Align and Translate(ICLR 2015)\n",
    "# 2021/04/16\n",
    "- seq2seq 모델에 attention을 추가한 `seq2seq with attention` 모델을 실습해보았다.\n",
    "- 기존의 seq2seq 모델은 인코더를 사용하여 context vector에 소스 문장의 정보를 압축하므로 이 과정에서 병목(bottleneck)이 발생하여 성능 하락의 원인이 된다.\n",
    "- 이를 해결하기 위해 `seq2seq with attention` 모델은 디코더가 단어를 하나 출력할 때마다 소드 문장의 출력 전부를 입력으로 받는다.\n",
    "- 최신 GPU는 큰 메모리 용량과 빠른 병렬 처리를 지원하므로, 소스 문장이 길더라도 각 단어의 인코더 출력값 전부를 특정 행렬에 기록하였다가 디코더가 각 단어를 출력할 때마다 소스 문장에 대한 정보를 입력해줄 수 있다.\n",
    "- 이 때 attention 매커니즘을 사용한다.\n",
    "- Attention 매커니즘에서는 디코더가 단어 하나를 출력할 때마다 출력 단어가 입력 문장에서 어떤 단어와 깊은 연관성을 가지는지를 학습한다.\n",
    "- 출력 단어와 연관성이 높은 단어를 소스 문장에서 가중치를 통해 찾아 해당 단어를 출력 단어로 번역한다.\n",
    "- 소스 문장의 단어와 출력 문장의 단어에 대한 연관성을 가중치(확률값)를 통해 표현하므로 학습이 완료되면, 어떤 단어가 어떤 단어와 연관성이 높아 번역되었는지 그리드 그래프로도 표현이 가능하다.\n",
    "- 이를 통해 짧은 문장은 물론이고, 단어 개수가 50개 이상인 매우 긴 문장에 대해서도 기존의 seq2seq 모델보다 우월한 성능을 보여주었다고 한다.\n",
    "- 이러한 seq2seq with attention 모델을 아래의 링크를 참조하여 구현해보았다.  \n",
    "  [papar: Neural Machine Translation by Jointly Learning to Align and Translate (ICLR 2015 Oral)](https://arxiv.org/abs/1409.0473)  \n",
    "  [Seq2Seq with attention + Transformer review](https://www.youtube.com/watch?v=AA621UofTUA)  \n",
    "  [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/nmt_with_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:01.834264Z",
     "start_time": "2021-04-19T08:08:59.078211Z"
    },
    "executionInfo": {
     "elapsed": 4403,
     "status": "ok",
     "timestamp": 1618798696792,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "YWG8aPtI6yJA"
   },
   "outputs": [],
   "source": [
    "# 사용할 패키지 import\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnzQZpgO6yJB"
   },
   "source": [
    "## 데이터 전처리\n",
    "- http://www.manythings.org/anki/ 에서 제공하는 언어 데이터셋을 사용한다.\n",
    "- 이 데이터셋은 영어-스페인어 번역의 쌍을 아래와 같은 형태로 포함한다고 한다.  \n",
    "    May I borrow this book? ¿Puedo tomar prestado este libro?\n",
    "- 데이터셋을 다운로드한 후에는 아래와 같은 전처리를 거친다.\n",
    "    1. 각 문장에 <sos>와 <eos> 토큰을 추가한다.\n",
    "    2. 특정 문자를 제거하여 문장을 정리한다.\n",
    "    3. 단어 인덱스와 아이디(ID) 인덱스를 생성한다(단어 -> 아이디, 아이디 -> 단어로 매핑된 딕셔너리).\n",
    "    4. 각 문장에 대해 입력층의 최대 길이만큼 패딩을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:01.926258Z",
     "start_time": "2021-04-19T08:09:01.835257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4402,
     "status": "ok",
     "timestamp": 1618798696794,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "bpt_4FtX6yJB",
    "outputId": "360a2b92-a252-4a4c-cddc-ea5ddc622044"
   },
   "outputs": [],
   "source": [
    "# 데이터셋 다운로드\n",
    "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "                                     extract=True)\n",
    "path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:01.942287Z",
     "start_time": "2021-04-19T08:09:01.927257Z"
    },
    "executionInfo": {
     "elapsed": 4400,
     "status": "ok",
     "timestamp": 1618798696794,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "n0zxwnDH6yJB"
   },
   "outputs": [],
   "source": [
    "# 유니코드 파일을 아스키 코드 파일로 변환\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    # 단어와 단어 뒤에 오는 구두점(.) 사이에 공백을 생성한다.\n",
    "    # 예시: \"he is a boy.\" => \"he is a boy .\"\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\")을 제외한 모든 것을 공백으로 대체한다.\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    \n",
    "    # 모델이 예측을 시작하거나 중단할 때를 알게 하기 위해 문장에 start와 end 토큰을 추가한다.\n",
    "    w = \"<start> \" + w + \" <end>\"\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:01.957281Z",
     "start_time": "2021-04-19T08:09:01.943260Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1618798696794,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "Z5uxtyXR6yJC",
    "outputId": "fff6b2ce-24b5-44d9-efbf-69e920053d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:01.972256Z",
     "start_time": "2021-04-19T08:09:01.958257Z"
    },
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1618798696795,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "fWVYjGyD6yJC"
   },
   "outputs": [],
   "source": [
    "# 1. 문장에 있는 억양을 제거한다. -> 마침표 같은 기호에 공백을 추가하여 문장이 \n",
    "# 어떤 기호로 끝나는지(억양)에 대한 내용을 제거한다는 말인 것 같다. \n",
    "# 2. 불필요한 문자를 제거하여 문장을 정리한다.\n",
    "# 3. word_pairs는 [영어, 스페인어] 형식으로 문장의 쌍을 리턴한다.\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "    \n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:06.219289Z",
     "start_time": "2021-04-19T08:09:01.973257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9876,
     "status": "ok",
     "timestamp": 1618798702282,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "kDHwx8cB6yJD",
    "outputId": "ca28aef2-67ae-4f24-e9ec-4e744ded93a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q05i5F96yJD"
   },
   "source": [
    "- 데이터셋으로 사용할 문장의 전체 개수는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:06.235257Z",
     "start_time": "2021-04-19T08:09:06.220257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9872,
     "status": "ok",
     "timestamp": 1618798702282,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "QIfvtVPJ6yJD",
    "outputId": "ab92d41b-adcb-449f-98da-1f5cee33b09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964\n"
     ]
    }
   ],
   "source": [
    "print(len(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:06.250259Z",
     "start_time": "2021-04-19T08:09:06.237261Z"
    },
    "executionInfo": {
     "elapsed": 9872,
     "status": "ok",
     "timestamp": 1618798702283,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "mjK5cg556yJE"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:06.265258Z",
     "start_time": "2021-04-19T08:09:06.252258Z"
    },
    "executionInfo": {
     "elapsed": 9870,
     "status": "ok",
     "timestamp": 1618798702283,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "blzU8Spl6yJE"
   },
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # 전처리된 타겟 문장과 입력 문장 쌍을 생성한다.\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J923apCS6yJE"
   },
   "source": [
    "#### 데이터 크기 제한(선택사항)\n",
    "- 현재 데이터셋의 문장은 10만개 이상인데, 이를 모두 훈련시키려면 시간이 매우 오래 걸리기 때문에 데이터셋의 크기를 \n",
    "3만개로 제한하는 코드이다.\n",
    "- 그러나 데이터셋의 크기는 클수록 좋기 때문에 일단 현재 상태에서 모델을 훈련시켜보고 시간이 너무 오래 걸린다 싶으면 크기를\n",
    "    줄여보려고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:07.936256Z",
     "start_time": "2021-04-19T08:09:06.266259Z"
    },
    "executionInfo": {
     "elapsed": 11051,
     "status": "ok",
     "timestamp": 1618798703465,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "tscP5nGT6yJE"
   },
   "outputs": [],
   "source": [
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:07.952286Z",
     "start_time": "2021-04-19T08:09:07.937257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11047,
     "status": "ok",
     "timestamp": 1618798703465,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "O5LpPcuL6yJF",
    "outputId": "d23b3d0a-4d5f-4cb6-cfb9-fba2c149f936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터셋과 검증 데이터셋을 8:2로 분리\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(X_train), len(y_train), len(X_valid), len(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:07.968286Z",
     "start_time": "2021-04-19T08:09:07.953257Z"
    },
    "executionInfo": {
     "elapsed": 11047,
     "status": "ok",
     "timestamp": 1618798703466,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "2yqAG9hz6yJF"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print(f\"{t} ----> {lang.index_word[t]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:07.984257Z",
     "start_time": "2021-04-19T08:09:07.969257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11043,
     "status": "ok",
     "timestamp": 1618798703466,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "EWyMD3m-6yJF",
    "outputId": "89574af3-63d6-42dc-c23f-b25c12abe576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input language; index to word mapping\n",
      "1 ----> <start>\n",
      "8 ----> no\n",
      "88 ----> hay\n",
      "221 ----> agua\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target language; index to word mapping\n",
      "1 ----> <start>\n",
      "57 ----> there\n",
      "8 ----> is\n",
      "66 ----> no\n",
      "258 ----> water\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input language; index to word mapping\")\n",
    "convert(inp_lang, X_train[0])\n",
    "print(\"\")\n",
    "print(\"Target language; index to word mapping\")\n",
    "convert(targ_lang, y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T07:30:04.115187Z",
     "start_time": "2021-04-17T07:30:04.106187Z"
    },
    "id": "MtDW-z4s6yJF"
   },
   "source": [
    "#### tf.data 데이터셋 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:08.514256Z",
     "start_time": "2021-04-19T08:09:07.985258Z"
    },
    "executionInfo": {
     "elapsed": 11042,
     "status": "ok",
     "timestamp": 1618798703466,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "WsYvbdN36yJF"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "embedding_dim = 256 # 논문에서는 620차원으로 하였다.\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder 옵션은 마지막 배치의 사이즈가 \n",
    "                                                         # BATCH_SIZE보다 작을 때 해당 배치를 훈련에 사용하지 않게 할 때 \n",
    "                                                         # 사용하는 옵션이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:08.560256Z",
     "start_time": "2021-04-19T08:09:08.515257Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11038,
     "status": "ok",
     "timestamp": 1618798703467,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "qJCg-Ehk6yJG",
    "outputId": "e13d619c-1984-4c46-a315-49c2fe05b288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 인코더와 디코더 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:08.576257Z",
     "start_time": "2021-04-19T08:09:08.561257Z"
    },
    "executionInfo": {
     "elapsed": 11037,
     "status": "ok",
     "timestamp": 1618798703467,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "2lokCdWd6yJG"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz): ## 선언 시 (전체 단어 수, embedding 차원, units, BATCH_SIZE)\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                       return_sequences=True, \n",
    "                                       return_state=True, \n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "#         print(x.shape)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:08.592258Z",
     "start_time": "2021-04-19T08:09:08.577257Z"
    },
    "executionInfo": {
     "elapsed": 11036,
     "status": "ok",
     "timestamp": 1618798703467,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "2jh4Acmn6yJG"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:09.718429Z",
     "start_time": "2021-04-19T08:09:08.593259Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1618798720542,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "L8yq6mIK6yJH",
    "outputId": "aa06aff8-0bac-4cc7-b558-8106fe55f9a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: [batch size, sequence length, units] (64, 16, 1024)\n",
      "Encoder hidden state shape: [batch size, units] (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# return_sequences=True로 지정하여, sample_output의 사이즈에 문장 전체 길이(sequence length)도 포함되어 있는 것을 \n",
    "# 알 수 있다.\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print(f\"Encoder output shape: [batch size, sequence length, units] {sample_output.shape}\")\n",
    "# 이게 의미하는 것은 인코더에 입력된 문장의 마지막 단어에 대한 hidden state이다.\n",
    "print(f\"Encoder hidden state shape: [batch size, units] {sample_hidden.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:09.733430Z",
     "start_time": "2021-04-19T08:09:09.719430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  2409984   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "=================================================================\n",
      "Total params: 6,348,288\n",
      "Trainable params: 6,348,288\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:09.748431Z",
     "start_time": "2021-04-19T08:09:09.734431Z"
    },
    "id": "NOjBeZOj6yJH"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W = tf.keras.layers.Dense(units)\n",
    "        self.U = tf.keras.layers.Dense(units)\n",
    "        self.v = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
    "        # query_with_time_axis shape: [batch size, 1, units]\n",
    "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
    "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
    "                                                             # 텐서를 리턴한다.\n",
    "        \n",
    "        # score shape: [batch size, max_length, 1]\n",
    "        # self.v() 이전 shape: [batch size, max_length, units]\n",
    "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
    "#         print(self.W(query_with_time_axis).shape)\n",
    "#         print(self.U(values).shape)\n",
    "#         print(score.shape)\n",
    "        # attention_weights shape: [batch size, max_length, 1]\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "#         print(attention_weights.shape)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DglvHFal6yJH"
   },
   "source": [
    "- 결과를 보면 attention_weights의 shape은 [batch size, sequence_length, 1]임을 알 수 있는데 이는 각 문장을 이루는 단어가\n",
    "현재 디코더의 출력 단어와 얼마나 연관성을 가지는지를 나타내는 확률값을 나타낸다.\n",
    "- 이를 인코더의 매 타임스텝마다의 hidden state를 의미하는 encoder output과 곱해 encoder output에 weight를 가한다.\n",
    "- 그리고 나서 reduce_sum을 하여 context_vector를 구한다.\n",
    "- 이를 통해 context_vector는 encoder의 output에 weight가 가해진 형태가 되어 입력 문장과 현재 출력 단어의 연관성에 대한\n",
    "정보를 가지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:09.934430Z",
     "start_time": "2021-04-19T08:09:09.749430Z"
    },
    "id": "UIDU61vj6yJI",
    "outputId": "53c137c6-dd16-415b-9fed-3324ffea5e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: [batch size, sequence_length, 1] (64, 16, 1)\n",
      "Attention result shape: [batch size, units] (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10) # 여기서 10이 의미하는 것은, 디코더가 출력하는 단어가 소스 문장과 얼마나 연관성이\n",
    "                                        # 있는지를 나타내는 attention_weigths를 계산할 때 사용할 fc층의 units이다.\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "print(f\"Attention weights shape: [batch size, sequence_length, 1] {attention_weights.shape}\")\n",
    "print(f\"Attention result shape: [batch size, units] {attention_result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:09.950433Z",
     "start_time": "2021-04-19T08:09:09.935432Z"
    },
    "id": "7iN5WfUZ6yJI"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # 어텐션을 사용한다.\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape: [batch_size, input_sequence_length, attention_units(= dec_units)]\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        # embedded x shape: [batch_size, 1, embedding_dim]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # concatenated x shape: [batch_size, 1, embedding_dim + attention_units]\n",
    "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)\n",
    "        \n",
    "        # 위에서 결합된 벡터 x를 GRU에 전달한다.\n",
    "        # output shape: [batch_size, 1, dec_units]\n",
    "        output, state = self.gru(x)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        # reshaped output shape: [batch_size * 1, dec_units]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # print(output.shape)\n",
    "        \n",
    "        # x shape: [batch_size, vocab_tar_size]\n",
    "        x = self.fc(output)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:10.013432Z",
     "start_time": "2021-04-19T08:09:09.951432Z"
    },
    "id": "9-bNWriK6yJI",
    "outputId": "d07da5cd-c7c7-49ab-da01-09b16b26dfc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: [batch_size, target vocab size] (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# example_target = tf.expand_dims(example_target_batch[:, 1], axis=1)\n",
    "# sample_decoder_output, _, _ = decoder(example_target, sample_hidden, sample_output)\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "print(f\"Decoder output shape: [batch_size, target vocab size] {sample_decoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T14:34:56.763584Z",
     "start_time": "2021-04-17T14:34:56.753584Z"
    },
    "id": "XxZxzary6yJJ"
   },
   "source": [
    "# 2021/04/18\n",
    "#### 최적화 함수와 손실 함수 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:10.028432Z",
     "start_time": "2021-04-19T08:09:10.014431Z"
    },
    "id": "n8kfYjF26yJJ"
   },
   "outputs": [],
   "source": [
    "# 논문에서는 optimizer로 Adadelta를 사용하였다.\n",
    "# optimizer = tf.keras.optimizers.Adadelta(rho=0.95, epsilon=1e-06)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0)) # 실제값 0은 inp_lang.index_word[0]을 하였을 때 존재하지 않는 단어\n",
    "                                                       # 이므로 loss로 사용하면 안된다. 실제값이 0인 경우 tf.math.equal()\n",
    "                                                       # 은 True를 리턴하고, tf.math.logical_not()은 True를 False로 변경\n",
    "                                                       # 하므로 mask에는 real이 0일 때 False가 입력된다.\n",
    "                                                       # 뒤에서 이 mask는 0으로 변경되고 loss_와 곱해져 loss_를 마스킹한다.\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype) # 데이터형을 변경해주는 함수이다.\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T05:12:43.694035Z",
     "start_time": "2021-04-18T05:12:43.689036Z"
    },
    "id": "H6vX2QgW6yJJ"
   },
   "source": [
    "#### 체크포인트(객체 기반 저장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:10.044432Z",
     "start_time": "2021-04-19T08:09:10.030431Z"
    },
    "id": "gfBrLLL56yJJ"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'chpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPRRv_6W6yJJ"
   },
   "source": [
    "#### 언어 모델 훈련하기\n",
    "1. 인코더의 output, 인코더 hidden state를 리턴하는 인코더에 소스 문장과 인코더 hidden state를 입력한다.\n",
    "2. 타겟 문장, 디코더 hidden state 및 어텐션을 통해 context vector를 구할 때 사용되는 인코더의 output을 디코더에 입력한다.\n",
    "3. 전달 받은 값을 사용하여 디코더는 매 타임스텝마다 예측 단어 및 디코더 hidden state를 리턴한다.\n",
    "4. 리턴된 디코더 hidden state는 다시 디코더에 입력되고 예측 단어를 사용하여 손실을 계산한다.\n",
    "5. 디코더에 대한 다음 타임스텝의 타겟 단어는 teacher forcing을 사용하여 결정된다.\n",
    "6. 마지막 단계에는 그레디언트를 계산하여 이를 optimizer와 역전파에 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:09:10.060431Z",
     "start_time": "2021-04-19T08:09:10.047434Z"
    },
    "id": "QFoAGgUW6yJJ"
   },
   "outputs": [],
   "source": [
    "# encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "# decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden # 인코더의 마지막 hidden state를 디코더의 첫번째 hidden state로 입력한다.\n",
    "        \n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, axis=1)\n",
    "        \n",
    "        # teacher_forcing: 다음 입력으로 타겟을 feeding한다.\n",
    "        for t in range(1, targ.shape[1]): # targ.shape[1]은 타겟 문장의 sequence_length가 된다.\n",
    "                                          # 훈련 시 디코더에는 타겟 문장의 \"<start>\" 토큰은 입력하지 않으므로 반복문을\n",
    "                                          # 1부터 시작한다.\n",
    "            # enc_output을 디코더에 전달한다.\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "            # teacher forcing을 사용하여 디코더의 predictions을 다음 입력으로 사용하는 것이 아니라 target 단어를 \n",
    "            # 다음 입력으로 사용한다.\n",
    "            dec_input = tf.expand_dims(targ[:, t], axis=1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1])) # 배치 문장 길이로 loss를 나누어 배치 내 각 문장의 loss 평균을 계산한다.\n",
    "        \n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:14:54.788046Z",
     "start_time": "2021-04-19T08:09:10.061430Z"
    },
    "id": "uU9qM9yk6yJK",
    "outputId": "38e0ea2f-2c45-4e0b-a81a-3ef9e875ff77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>>, which Python reported as:\n",
      "    def call(self, x, hidden):\n",
      "        x = self.embedding(x)\n",
      "#         print(x.shape)\n",
      "        output, state = self.gru(x, initial_state = hidden)\n",
      "        return output, state\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>>, which Python reported as:\n",
      "    def call(self, x, hidden):\n",
      "        x = self.embedding(x)\n",
      "#         print(x.shape)\n",
      "        output, state = self.gru(x, initial_state = hidden)\n",
      "        return output, state\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>>, which Python reported as:\n",
      "    def call(self, x, hidden):\n",
      "        x = self.embedding(x)\n",
      "#         print(x.shape)\n",
      "        output, state = self.gru(x, initial_state = hidden)\n",
      "        return output, state\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Encoder.call of <__main__.Encoder object at 0x0000023AC150CE88>>, which Python reported as:\n",
      "    def call(self, x, hidden):\n",
      "        x = self.embedding(x)\n",
      "#         print(x.shape)\n",
      "        output, state = self.gru(x, initial_state = hidden)\n",
      "        return output, state\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x0000023AC1268B88>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        # query: 디코더의 매 타임스텝마다의 hidden state, query hidden state shape: [batch size, units]\n",
      "        # query_with_time_axis shape: [batch size, 1, units]\n",
      "        # values: 인코더의 출력(hidden state), values shape: [batch size, sequence_length, units]\n",
      "        query_with_time_axis = tf.expand_dims(query, axis=1) # expand_dims()는 입력한 axis에 차원을 1만큼 증가시킨 \n",
      "                                                             # 텐서를 리턴한다.\n",
      "        \n",
      "        # score shape: [batch size, max_length, 1]\n",
      "        # self.v() 이전 shape: [batch size, max_length, units]\n",
      "        score = self.v(tf.nn.tanh(self.W(query_with_time_axis) + self.U(values)))\n",
      "#         print(self.W(query_with_time_axis).shape)\n",
      "#         print(self.U(values).shape)\n",
      "#         print(score.shape)\n",
      "        # attention_weights shape: [batch size, max_length, 1]\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "#         print(attention_weights.shape)\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "        \n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "Epoch 1 Batch 0 Loss 4.4458\n",
      "Epoch 1 Batch 100 Loss 2.2113\n",
      "Epoch 1 Batch 200 Loss 1.8173\n",
      "Epoch 1 Batch 300 Loss 1.8344\n",
      "Epoch 1 Loss 2.0153\n",
      "Time taken for 1 epoch 49.82225275039673 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.5501\n",
      "Epoch 2 Batch 100 Loss 1.4945\n",
      "Epoch 2 Batch 200 Loss 1.3230\n",
      "Epoch 2 Batch 300 Loss 1.2534\n",
      "Epoch 2 Loss 1.3424\n",
      "Time taken for 1 epoch 33.269607067108154 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9919\n",
      "Epoch 3 Batch 100 Loss 1.0526\n",
      "Epoch 3 Batch 200 Loss 0.9376\n",
      "Epoch 3 Batch 300 Loss 0.9125\n",
      "Epoch 3 Loss 0.9113\n",
      "Time taken for 1 epoch 32.85926675796509 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.6624\n",
      "Epoch 4 Batch 100 Loss 0.5799\n",
      "Epoch 4 Batch 200 Loss 0.6140\n",
      "Epoch 4 Batch 300 Loss 0.5704\n",
      "Epoch 4 Loss 0.6013\n",
      "Time taken for 1 epoch 33.375417947769165 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.3224\n",
      "Epoch 5 Batch 100 Loss 0.4072\n",
      "Epoch 5 Batch 200 Loss 0.3825\n",
      "Epoch 5 Batch 300 Loss 0.3714\n",
      "Epoch 5 Loss 0.3998\n",
      "Time taken for 1 epoch 32.37506127357483 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.2646\n",
      "Epoch 6 Batch 100 Loss 0.2154\n",
      "Epoch 6 Batch 200 Loss 0.2692\n",
      "Epoch 6 Batch 300 Loss 0.2904\n",
      "Epoch 6 Loss 0.2751\n",
      "Time taken for 1 epoch 32.74703812599182 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1404\n",
      "Epoch 7 Batch 100 Loss 0.1749\n",
      "Epoch 7 Batch 200 Loss 0.1964\n",
      "Epoch 7 Batch 300 Loss 0.1843\n",
      "Epoch 7 Loss 0.1954\n",
      "Time taken for 1 epoch 32.3191032409668 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1321\n",
      "Epoch 8 Batch 100 Loss 0.1122\n",
      "Epoch 8 Batch 200 Loss 0.1411\n",
      "Epoch 8 Batch 300 Loss 0.1297\n",
      "Epoch 8 Loss 0.1445\n",
      "Time taken for 1 epoch 32.815184593200684 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1252\n",
      "Epoch 9 Batch 100 Loss 0.1291\n",
      "Epoch 9 Batch 200 Loss 0.1396\n",
      "Epoch 9 Batch 300 Loss 0.1040\n",
      "Epoch 9 Loss 0.1136\n",
      "Time taken for 1 epoch 32.31700038909912 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0759\n",
      "Epoch 10 Batch 100 Loss 0.0930\n",
      "Epoch 10 Batch 200 Loss 0.0793\n",
      "Epoch 10 Batch 300 Loss 0.0761\n",
      "Epoch 10 Loss 0.0949\n",
      "Time taken for 1 epoch 32.81363129615784 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1} Batch {batch} Loss {batch_loss.numpy():.4f}\")\n",
    "        \n",
    "    # 에포크가 2번 실행될 때마다 모델 저장(체크포인트)\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} Loss {(total_loss / steps_per_epoch):.4f}\")\n",
    "    print(f\"Time taken for 1 epoch {time.time() - start} sec\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-18T06:23:25.243588Z",
     "start_time": "2021-04-18T06:23:25.226587Z"
    },
    "id": "Dq8OcMox6yJK"
   },
   "source": [
    "#### 훈련된 모델로 번역하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:44.712407Z",
     "start_time": "2021-04-19T08:15:44.693377Z"
    },
    "id": "rU8qP2T_6yJK"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "#     print(inputs[0])\n",
    "#     for input in inputs[0].tolist():\n",
    "#         if input == 0:\n",
    "#             continue\n",
    "#         print(inp_lang.index_word[input])\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))] # 문장 1개에 대한 hidden state이므로 shape이 [1, units] 이다.\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index[\"<start>\"]], axis=0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # 어텐션 가중치를 시각화하기 위해 어텐션 가중치를 저장한다.\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "#         print(result)\n",
    "#         print(attention_weights)\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # 예측된 ID를 모델에 다시 feed한다.\n",
    "        dec_input = tf.expand_dims([predicted_id], axis=0)\n",
    "        \n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:44.728378Z",
     "start_time": "2021-04-19T08:15:44.714401Z"
    },
    "id": "jjxcgDFH6yJK"
   },
   "outputs": [],
   "source": [
    "# 어텐션 가중치를 plot하기 위한 함수이다.\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:44.744402Z",
     "start_time": "2021-04-19T08:15:44.729389Z"
    },
    "id": "sR4MzVi-6yJK"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print(f\"Input: {sentence}\")\n",
    "    print(f\"Predicted translation: {result}\")\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zNNrbOE6yJL"
   },
   "source": [
    "#### 마지막 체크포인트를 복원하고 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:45.846409Z",
     "start_time": "2021-04-19T08:15:44.745378Z"
    },
    "id": "u13I-Gy66yJL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x23e65f6bb88>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:46.080408Z",
     "start_time": "2021-04-19T08:15:45.847380Z"
    },
    "id": "YJWqjObq6yJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it is very cold here . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:46.285377Z",
     "start_time": "2021-04-19T08:15:46.082381Z"
    },
    "id": "b15XY8kX6yJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:46.515377Z",
     "start_time": "2021-04-19T08:15:46.286381Z"
    },
    "id": "0fPaeC-r6yJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Predicted translation: are you still at home ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAJwCAYAAAAjo60MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAApn0lEQVR4nO3debiu93zv8c832RmaRJAgYh5abcxi10xpSujgHOpQc6ikiPmgx6mpxtbUUnoINRNDVFE1RHGo4SgORUwhaRARIUTInO/54372yVrL3pGd7L3u39rr9bqufe1n3c+z1vqu+0rW8973WN0dAADmt9PcAwAAMBFmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmA6iq36iqD1fVDeaeBQCYjzAbwwOT3D7Jg2eeAwCYUbmJ+byqqpIcn+ToJH+U5Erdfd6sQzGMqrpikl2XLuvuE2YaB4DtzBaz+d0hyaWSPCrJuUl+f95xmFtVXbqqXldVZyT5XpLjVvwBYAclzOb3gCRHdfcvkhyZabcm69sLktwoyX9NcmaS+yR5QpLvJrnXfGMBsL3ZlTmjqtozyfeT/EF3f7yqbpzkU5l2Z54663DMpqq+m+Tei/8mTktyYHcfW1X3TvLg7r7jzCMCsJ3YYjavP05ySnd/PEm6+wtJvpnkT+YcitldJsl/Lh7/NMm+i8efSnKrOQYCWOuqas+qekBVXXruWS6MMJvX/ZO8ccWyN8buzPXuW0mutXj81SR/sjhJ5O5JfjzbVABr2z2TvCbTe++w7MqcSVVdNdOB3Ad09zeXLL9KprM0r9vd35hpPGZUVY9Ncl53v6SqfjfJPyfZJdM/pB7d3S+ddUCANaiqPprkCkl+0d0bZx5ni4QZDK6qrpZkY5JvdveX5p4HYK2pqmsk+UaSmyX5dKZjd4+ZdagtsCtzRlV1tcUuqs0+t9rzMKbuPqG7/1GUAVxs90/y8cWx3P+SgQ8ZssVsRlV1XpL9u/vkFcv3TXJyd+88z2Sstqp6XJK/7+4zF4+3qLtftEpjAewQquqbSZ7d3a+tqrsneUmSq/aAESTMZlRV5yfZr7t/uGL51ZMc0917zjMZq62qjkuysbt/tHi8Jd3d17qQ5wFYoqpuleSDmd5vf15VuyY5Kcm9uvvoeaf7ZRvmHmA9qqqXLB52kudW1S+WPL1zpn3gX1jtuZhPd19zc48BuMQemORd3f3zJOnus6vqbUkOyXQ7xKEIs3ncYPF3JTkgydlLnjs7yeczXf2ddaiqbtTdX5x7DoC1rqp2y3SZjHuveOqNST5QVXt19+mrP9mW2ZU5k8VB/2/LdCX3n809D+NY7OL+SpI3JDmyu78z80gAa1JVXS7TPajf2N3nr3jufkk+1N0nzTLcFgizmVTVzpnug3ijUU/ZZR5VdZ0k9830L7xrJfl4pkg7qrtPm3O2uVTV7kkeneSgTNchWnZGeXffcI65ALY1YTajqjo2yT0Wp+/CL6mqm2eKtHsm2TvJP3f3PeedavVV1auT3C3J25OcmOn4zP+vu/9yjrkAtjVhNqOqemCmrSL36+5T5p6HcS0C7eVJbrgeL6NSVT9Ocs/u/tDcswDjW5zdfpECZ7Qz3R38P6/HJ7lmku9V1XeT/Hzpk3bPrG9Vda0k98m0xezXM+3SfMisQ83nF0kcawdcVEtvXbdXkscl+UySTy2W3TLTFRBeuMpz/Uq2mM2oqp52Yc/bPbM+VdXhmWLs5km+nORNSd7U3d+bdbAZVdWjklwvycNWHsALcGGq6rVJvtHdz1mx/ElJrtfd95tlsC0QZjCYqvpOkiOTvMFtmCZV9Z4kt03y0yTHJDln6fPdfdc55gLGV1WnZbo35rErlv96ks93997zTLZ5dmXCeK424m1CZnZKknfOPQSwJv08ye2THLti+e0zHSYxFGE2o8VtIf4i0wkAV0uyy9Ln1+NB3kz3XEqSqrpSpv8udl3x/MfmmGtO3f2guWdgXH6X8iv8TZKXVdXGJJ9eLLtFpjsCPH2uobZEmM3rmUnuleS5mf7DeUKSayT5kyRPmW8s5rQIsiMz7brrTHeIWLoFzZsMLOd3KVvU3c+rquMzXQtx0+WGvprkgd39ttkG2wLHmM1ocTrvw7r7/VX1syQ37u5vVdXDkhzU3feYeURmsLiH275JDk/y70nunGS/JM9I8tgRb7q7GqrqQblgi8jKrYhDne7O6vK7lB3JTr/6JWxH+2U6kDlJTk9ymcXj9ye50xwDMYTfSfLn3f21TFvKftjd/5jkzzNtGVh3quoJmU5r/1ymLSH/lOmM1X2SvHq2wRiF36VcJFV1maraZ+mfuWdaSZjN64QkV1o8PjbJwYvHt0xyxiwTMYJfy3Swe5L8ONMtiJLpjWe9Xtvu0CSHdfeTMp2R+dLFmZgvTHL1WSdjBH6XskVVdfWqel9VnZnkR0l+uPhzyuLvoTjGbF7vzHTvv08neXGSI6vq0CRXTvL8OQdjVl9L8ltJjk/yhSQPXVxC4/Ak6/VaZlfJdHHIZHqj3XR6+5GL5YfOMRTD8LuUC/OaTFtRH5zN3NJtNI4xG8jitju3znQhvH+eex7mUVX3TbJLd7+2qg7MtDtm3yRnZTpY9e2zDjiDqvp2pvvKfr6q/j3Jq7v7f1XVnTNdfHffmUdkIFV1iyS3it+lJKmq05Pcoru/PPcsF4Uwm1FV3S7JJ7v73BXLNyS51Xq8LAK/rKr2yLQF7YT1ek/VqnpVku9299Or6qGZzrz7dJIDk7ytu20xAzarqr6U5JDu/tzcs1wUwmxGVXVekv27++QVy/dNcrJr78CkqnZKstOmf8RU1b2y2Lqc5BXdfc6FfT47tqq6Z5KfdPcHFx8/NclhSb6S6Q35+3POx7yq6neT/I8kD1959f8RCbMZVdX5Sfbr7h+uWH6dJJ8d7TYRbD9VdZHPLOzuB2/PWUZUVVdL8p2Vd0Soqkpy1e4+YZ7JGEFVHZPkMd39wcXu/08meWqmS82c1N33mXVAZrW4hMpuma4BeVaSZXupRnuvdfD/DKrq3YuHneSNVXXWkqd3TnL9TL9YWD8uv+Lj2yU5P8mme2VeP9NZ1Ot19/ZxSfZPcvKK5fssnrN1eX27epKvLx7fLck/LS4q+sEkH5hvLAbxiLkH2BrCbB4/WvxdSU7N8tO5z07yb0leudpDMZ/u/qNNj6vqSZn+m3hQd/98sWzPJP+QC0JtvVl594NN9kpy5irPwnjOTHKpxeODcsG17X66ZDnrVHe/bu4ZtoZdmTOqqqclecGmN19Ikqr6fqarlR+zYvn1kvxrd19xnslWX1W9ZPHw8EynvC+94fDOSW6W5OzuvvVqz8Y4quqfMl3/798y3YLpGt19YlUdnOQl3f2bc87H/KpqvyT3T3LtJE/p7lOq6tZJTuzu4+adbjkXmJ3XM7Nka1lVXbGqHlJVt5pxJua3Vy64WOZS+yfZY5VnmdsNFn8qyQFLPr5Bkl9P8vkkh8w1HMN4RKa9DfdI8tDuPnGx/C6xK3Pdq6qbZtrVfd8kf5oLroN4xyTPnmuuLbHFbEZV9b4k7+/uF1fVXpkuLLpnpjfmP+3u1886ILOoqtdm2h3zhEyXhEiSWyT56yQf6e5D5plsPlX1miSP7u7T5p5lFIvLqNw4050hlv0je3ELLyBJVX0kyce6+2mLEwFu1N3frqpbJnlLdw919xBhNqOqOjnTLqsvVdUDMp3Oe6NMVf+47l6vt99Z16rq1zLdaujBSXZZLD430zFmj+/uX2zpc9eLxTq6dZJvdvd/zj3Paquq38t014PNXVi3XWoHLlBVp2W6sf23V4TZNZJ8rbt3n3fC5ezKnNelkvxk8fhOSd65uB7ThzPtB2cd6u4zuvvhmd50b5LpIqr7dPfD12uUVdVrq+rhi8e7ZroN0weTfL2q7jLrcPN4cZL3JrlKd++04s+6i7Kq2rWq/rKqvlFVZ1bVeUv/zD0fszsjyWU3s/y38stnes9OmM3rhCS3Xpxxd3CSoxfL98nyg5xZn87LdMmMcxeP17ODc8Fu3btm+kfNFZM8ffFnvblGkmcuOZZqvXtmkgdm2tJ8fqbDAF6W6Qz4h884F2N4V5KnVdVui497sbXsr5O8Y7aptkCYzetFSd6Q5LuZbk696RpVt8v6vSzCuldVG6rq+ZkupfLFTP8tnFpVz6uqXS78s3dYl80F/7K9c5J3LO6Y8ZYk151tqvl8IokzDS9wz0wH/b8i0z9i3tXdj0rytEwHeLO+PT7TBo8fZjqB6t+SHJvpcipPnnGuzXIdsxl19yuq6rNJrpbk6O4+f/HUtzKd8s369Lwk907y0Ey/QJLktkmem+kfU4+faa45nZTk+otLiRyc6XY7yXSizHq8HdPLk7ygqq6UKdyXrYPu/vwsU81nvySbLi9zepLLLB6/P9NWEdaxxUlDt1ncmunATL9HP9/dH5p3ss0TZjOpqksnuWF3fzzJyhur/iQX/JJh/blPkgd3978sWfatqvphkldlfYbZq5O8NcmJmbaI/Oti+c0znc283hy1+PuIzTzXWX93Qjgh0yVmTsi0JeTgTL9Xb5nlF/BmnVn6XtvdH850DPem526d5JjuPnW2ATdDmM3n/CTvq6qDu/sTmxZW1Y0z/Ydz5bkGY3aXzrTVdKVv5YItAetKdz+jqr6c6dY7b+vusxdPnZv1uUXkmnMPMJh3ZrrEzKcznRhxZFUdmun36PPnHIzZrbn3WseYzaS7f5bpgMQHrHjqfkk+0N2nrP5UDOKLSR61meWPTvKF1R1lKGck+b0kR1fVVRfLds2062pdWVwi5LqZDnB/X5LzF8vumOnCu+tKdz+pu5+9eHxUktsk+bskd+/uv5h1OGa1Ft9rhdm8Xp/kv206oLuqdsq0G+u1cw7F7J6Y5IGLU/9ft7hUxNcz/SJ5wsyzzaKq7pvkbUm+kWlr0aaTIHbKtL7WlSXr45tZvj52zvpcH8+uqodu+ri7/093vyjJVarqmTOOxhjW1HutMJvX0Zkui7HpBtYHZdoC8J7ZJhpUVe1cVYdX1XrYhXN8kuskeXumg9v3Xjz+zUzH0KxHT0xyaHc/NtPuy00+nenq9+uN9bHc/ZP8380s/1x+eUvJDq2q/rCqHra4NySTNfVeK8xmtDgL80254BfH/ZO8dXGRWZbo7vOSXD/JM+aeZRUcl+Tc7v6L7v7j7r57dz85yVmL59aj30jyqc0sPz0X3PduPbE+lrtCpkshrPSjTGdsrgtV9T8yHW/35CT/UVU3mHmkIay191phNr/XJ7nz4piZuyV53czzzKKqPlJVr6mqyy4ev7uqHrjiZW9J8rtzzLfKKtOZdSvtleTMVZ5lFCdm2oq40u2y+RMldnTWx3InZLqkzEq3y3SdyPXi4Znus3zlTCdBHF1Vd6qqqy2uj7h/VV1t5hnnsmbea52VObPu/kpVfSnJm5N8t7s/M/dMM/lypmtVnbN4fKkkL6uqmy4uFJkkZ2eKkx1SVb1k8bCTPLeqlt79YeckN8v6Pfj/iCQvqaqHLD6+alXdNtM1354+21TzsT6We0WSv1ncrmvT5RAOynTtv/V01u4+WVyovLufsziW6n2L534701aj62T9XU5lTb3XCrMxvCHJ3yZZt2cPdfcjl3z4yCSpqr9L8v6qunqSf0zyiCQfn2G81bJpt0MlOSBTiG5ydpLPJ3nBag81gu5+3uJ6REcn2T3JRzLt2n1Bd79s1uFmYH0s190vrKrLJXlJpmOHkun/mRd39/Pmm2zVfSPT2brHJ0l3P6uq/j7TLby+mmlX3h5zDTeANfFeW92b22PCaqqqfTLFyCu6+6S55xlJVV0nyd9n+tfeZ5Mc0t3fmXeq7auqXpPk0YurVbNEVe2R6Y1np0wXhlx3l8pYyvpYbnHf4etm+sfNulsfVfWIJHfo7j+ee5YRrZX3WmEGADAIB/8DAAxCmAEADEKYDaKqDpt7hpFYH8tZH8tZH8tZH8tZH8tZH8uNvj6E2TiG/g9lBtbHctbHctbHctbHctbHctbHckOvD2EGADCIdX9W5q61W++ePeceI+fkrOyS3eYeYxjWx3LWx3LWx3LWx3LWx3LDrI+ae4DJOX1Wdqn518fP+tRTuvvyK5ev+wvM7p49c/M6aO4xAGCHVhvWfXIsc/Q5b/nPzS23KxMAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEGs+zKpql7lnAADYFoYLs6q6c1V9vKpOraofV9UHquqAxXPXqKquqntX1Yer6owkf7Z47kFVdUxVnVlV36iqx1bVcD8fAMCWbJh7gM3YM8nfJvmPJL+W5MlJ3lNV113ymucmeXySP01yTlUdmuQZSR6Z5HNJrp/klUnOSfLSVZscAOASGC7MuvsdSz+uqgclOS3JzZJ8d7H477r7qCWveUqSJy5ZdlxV/VWSh2czYVZVhyU5LEl2zx7b/GcAALg4hguzqrp2kmcmuXmSy2fa3bpTkqvlgjD77JLXXz7JVZO8oqr+15IvtSFJbe57dPcRSY5Ikr1rn97GPwIAwMUyXJgleU+S72U6dux7Sc5NckySXZe85udLHm86juyhST65GgMCAGwPQ4VZVe2b5IAkh3f3RxbLDsyFzNndP6iq7yW5dne/fnUmBQDY9oYKsySnJjklyaFV9Z0kV07y/ExbzS7M05P8XVX9JMm/JNklyYFJrtzdz91u0wIAbENDXU6iu89Pcq8kN0zy5SQvS/KUJGf9is97VZIHJ7l/ki8m+Ximg/uP257zAgBsS6NtMUt3fzjT5S6W2mvJ4y0d0H9kkiO311wAANvbUFvMAADWM2EGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADAIYQYAMAhhBgAwCGEGADCIDXMPMLeqyk677z73GOPYSasvddyf33juEYay54k99whD2e+tx8w9wljK74+lzv/Zz+YeYSh97rlzj7Am+L8IAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBCzh1lVPaCqflRVu61Y/qaqevfi8Z9V1bFVdfbi70NXvLar6h4rlh1fVY/f/j8BAMC2MXuYJXl7pjn+y6YFVXXpJHdL8g9VdbckL03yt0mun+TFSf6+qv5o9UcFANh+Nsw9QHefUVVvSvLgJG9bLL5PktOSvDfJ/07yhu5+6eK5b1TVTZP8eZL3XJzvWVWHJTksSXavPS/B9AAA284IW8yS5JVJ7lhVV1l8/OAkr+vuc5MckOQTK17/b0mue3G/WXcf0d0bu3vjrtntV38CAMAqGCLMuvuLST6f5JCqun6SjUlevfQlm/u0FY9rxfO7bNMhAQC2syHCbOGVSQ5J8pAkn+jury+WfzXJbVa89jZJjlny8Q+T7L/pg6rab+nHAABrwezHmC1xZJIXJXlYkocuWf78JG+vqs8l+WCSOye5b5K7L3nNh5McXlWfTHJekuckOXM1hgYA2FaG2WLW3T/LdPD/2bngJIB09z8leWSSx2baSvboJA/v7qUH/v/3JN9O8tEkRyV5VZKTV2NuAIBtZaQtZsm0+/Et3f3zpQu7++VJXr6lT+ruE5PcZcXid2z78QAAtp8hwqyq9knye0nulORGM48DADCLIcIs0xmZ+yT5n9395bmHAQCYwxBh1t3XmHsGAIC5DXPwPwDAeifMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGsWHuAebW3Tn/zDPnHoNBXf3pn5p7hKGcfPgt5x5hKN9+3HXnHmEoV/z0uXOPMJQ9PnXs3CMM5bxTT517hDXBFjMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQazZMKuqj1bVSy/qxwAAo9sw9wC/SlUdkuSl3b3XiqfunuSc1Z8IAGD7GD7MtqS7fzz3DAAA29IwuzKr6nZV9emqOr2qflpV/6eqHpHkNUn2rKpe/Hn64vV2VQIAO5QhtphV1YYk70ryD0num2SXJAcm+UqSxyR5TpJrL15++gwjAgBsd0OEWZK9k1wmyXu6+1uLZV9Lkqq6SZLu7pO21TerqsOSHJYku2ePbfVlAQAukSF2ZS6OF3ttkg9U1Xur6nFVddXt+P2O6O6N3b1xl+y2vb4NAMBWGSLMkqS7H5Tk5kk+luSuSb5RVQfPOxUAwOoZJsySpLu/2N1/3d23T/LRJA9McnaSneecCwBgNQwRZlV1zar6q6q6VVVdvarukOSGSY5JcnyS3avqjlV1uapyUBgAsEMa5eD/XyS5TpK3J7lckh8keVOSv+7uc6rq5UmOTLJvkr9M8vSZ5gQA2G6GCLPu/kGmK/lv6fmHJXnYimW335qPAQBGN8SuTAAAhBkAwDCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCA2zD0ADK177gmGcoWXfnLuEYZSH77y3CMM5Refsz6W6qvuN/cIY/nJT+aeYCxbeHuxxQwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQO1yYVdU1qqqrauPcswAAbI0dLswAANaqNRlmVXXnqvp4VZ1aVT+uqg9U1QGLp49b/P3viy1nH51pTACArbImwyzJnkn+NsnNktw+yU+TvKeqdl0sS5I7J9k/yd1nmA8AYKttmHuAi6O737H046p6UJLTMkXZdxeLf9TdJ23u86vqsCSHJcnu2WM7TgoAcNGtyS1mVXXtqnpzVX2rqk5L8oNMP8vVLsrnd/cR3b2xuzfukt2266wAABfVmtxiluQ9Sb6X5M8Wf5+b5Jgku845FADAJbHmwqyq9k1yQJLDu/sji2UH5oKf5ezF3zvPMB4AwMW25sIsyalJTklyaFV9J8mVkzw/01azJDk5yRlJDq6q45Oc2d0/nWNQAICtseaOMevu85PcK8kNk3w5ycuSPCXJWYvnz03yqCQPSXJiknfNMykAwNZZi1vM0t0fTnL9FYv3WvL8q5K8alWHAgC4hNbcFjMAgB2VMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGMSGuQcAWKv6oBPnHmEoe2z44dwjDOUmnzlz7hGG8rFn3XLuEcZy1Ns3u9gWMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBCDMAgEEIMwCAQQgzAIBBbNMwq6qPVtVLt+XXBABYL2wxAwAYhDADABjE9giznarqOVV1SlWdXFUvqKqdkqSqLltVr6uqU6vqjKr6UFVdb9MnVtUhVXV6Vd2lqr5WVb+oqndX1aWr6h5V9c2q+mlVvaGqfm3J51VVPbGqvrX4ul+qqvtth58NAGC72R5hdt8k5ya5VZJHJHlMknstnnttkpsn+S9JbpbkF0nevzSykuyW5L8vvs5BSTYmOSrJA5P8cZL/muQPkzx8yec8K8mfJjk8yXWTPDfJK6rqDzY3YFUdVlWfrarPnpOzLtEPCwCwrWzYDl/zmO5+6uLxN6rq0CQHVdVnk9w1ye9098eSpKrun+SETBH2qiUzHd7dX1+85s1JHptkv+4+ZbHsXUnukOSFVbVnkscluVN3f3zxNY6rqptlCrX3rhywu49IckSS7F379Db96QEALqbtEWb/seLjE5NcIckBSc5P8qlNT3T3T6vqS5m2cm1y1qYoW/hBkpM2RdmSZZs+57pJds+05W1pZO2S5PhL8HMAAKyq7RFm56z4uDPtMq0L+ZylQXXuZp7b0tfMkr//KNPWtwubBQBgWNsjzLbkmEwRdcskm3Zl7p3kBklecwm/7llJrt7dH76kQwIAzGXVwqy7v7k4NuwVVXVYkp8keXaS05K8+RJ83Z9V1QuSvKCqKlP07ZXkFknOXxxPBgAwvNW+jtmDknwmybsXf++R5M7dfcYl/LpPSfL0JI9P8pUkR2c6g/O4S/h1AQBWTXWv75MS9659+uZ10NxjAGtRXdihs+tPbdhl7hGGcuBnzpx7hKF87Fm3nHuEoXzqqCd8rrs3rlzuyv8AAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAgxBmAACDEGYAAIMQZgAAg9gw9wDAGlI19wQMrM89Z+4RhvLRv7rV3CMMZeOTPjf3CEP51FGbX26LGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCB2qDCrqkdU1f+tqp9X1Xeq6klzzwQAcFFtmHuAbeygJE9N8pUkt0vyqqr6Sne/e96xAAB+tR0qzLr7bks+/HZVPSfJVeeaBwBga+xQYbZUVf3PJLsk+cfNPHdYksOSZPfsscqTAQBs3g51jNkmVfXkJI9Jcsfu/v7K57v7iO7e2N0bd8luqz4fAMDm7HBbzKpq3yTPSPIH3f2FmccBALjIdsQtZtdIUkm+OvMcAABbZUcMs68m+e0kJ849CADA1tgRw+z6Sd6Y5PJzDwIAsDV2xDDbI8lvZjojEwBgzdjhDv7v7o9mOsYMAGBN2RG3mAEArEnCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEMIMAGAQwgwAYBDCDABgEBvmHgBYQ7rnngDWjMt+7Pi5RxjKS/7m3+ceYSgv28JyW8wAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABiHMAAAGIcwAAAYhzAAABrFmwqyqHl9Vx889BwDA9rJmwgwAYEe3TcKsqvauqstsi6+1Fd/z8lW1+2p+TwCA7elih1lV7VxVB1fVm5OclORGi+WXrqojqurkqvpZVf3vqtq45PMOqarTq+qgqvpyVf28qj5SVddc8fWfWFUnLV77+iR7rRjh95OctPhet764PwcAwCi2Osyq6npV9bwkJyR5a5KfJ7lzko9VVSV5b5IrJ/nDJDdJ8rEkH66q/Zd8md2SPCnJg5PcMsllkrx8yfe4Z5JnJXlakgOTfD3J41aM8qYk90lyqSRHV9WxVfXUlYG3hZ/hsKr6bFV99pyctZVrAABg+6ju/tUvqto3yX2TPCDJDZO8P8kbkry7u89a8rrfTfLuJJfv7jOWLP9Ckjd39/Oq6pAkr0nyW9399cXz910s2727z6+qTyb5SncfuuRrfCjJr3f3NTYz36WS/Lck909y2ySfSPK6JG/r7tMv7Gfbu/bpm9dBv3IdAMDW2LD/FeceYSjv/dz75x5hKDvvf+znunvjyuUXdYvZI5O8OMlZSX6ju+/a3W9fGmULN02yR5IfLnZBnl5Vpye5fpJrL3ndWZuibOHEJLtk2nKWJAck+dSKr73y4/+vu3/W3a/u7jsk+e0kV0jyD0nucRF/PgCA2W24iK87Isk5mbaYfaWq3plpi9m/dvd5S163U5IfZNpqtdJpSx6fu+K5TZvtLtYxb1W1W5I/yLTF7PeTfCXJY5K86+J8PQCAOVykEOruE7v72d39m0l+L8npSd6S5LtV9cKqusnipZ9Psl+S87v72BV/Tt6Kub6a5BYrli37uCa3qapXZDr54KVJjk1y0+4+sLtf3N2nbsX3BACY1VZvoeruT3f3w5Lsn2kX53WSfKaqbpvkQ5mO73pXVd2lqq5ZVbesqr9cPH9RvTjJA6vq0Kr6jap6UpKbr3jN/ZJ8MMneSe6d5Krd/YTu/vLW/kwAACO4qLsyf8ni+LKjkhxVVVdIcl53d1X9fqYzKl+Z6VivH2SKtddvxdd+a1VdK8mzMx2z9u4kL0pyyJKX/WuSK3b3ab/8FQAA1p6LdFbmjsxZmQBsD87KXM5Zmctd0rMyAQDYzoQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAgNsw9AADsiM79/klzjzCUg69047lHGMyxm11qixkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAghBkAwCCEGQDAIIQZAMAgNsw9wByq6rAkhyXJ7tlj5mkAACbrcotZdx/R3Ru7e+Mu2W3ucQAAkqzTMAMAGJEwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYhDADABiEMAMAGIQwAwAYRHX33DPMqqp+mOQ/554jyeWSnDL3EAOxPpazPpazPpazPpazPpazPpYbZX1cvbsvv3Lhug+zUVTVZ7t749xzjML6WM76WM76WM76WM76WM76WG709WFXJgDAIIQZAMAghNk4jph7gMFYH8tZH8tZH8tZH8tZH8tZH8sNvT4cYwYAMAhbzAAABiHMAAAGIcwAAAYhzAAABiHMAAAG8f8AJyKlUiSkHJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-19T08:15:46.705472Z",
     "start_time": "2021-04-19T08:15:46.517378Z"
    },
    "id": "0Obh78876yJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> trata de averiguarlo . <end>\n",
      "Predicted translation: try to find out . <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  if __name__ == '__main__':\n",
      "D:\\Anaconda\\envs\\tf2.0\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 잘못된 번역\n",
    "translate(u'trata de averiguarlo.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Machine Translation by Jointly Learning to Align and Translate(ICLR 2015).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
