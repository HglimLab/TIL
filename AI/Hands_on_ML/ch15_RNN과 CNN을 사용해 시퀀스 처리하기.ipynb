{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T06:28:44.027820Z",
     "start_time": "2021-04-04T06:28:30.826142Z"
    },
    "executionInfo": {
     "elapsed": 4302,
     "status": "ok",
     "timestamp": 1617793271019,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "0xsg70m2_ePR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOulP516_ePp"
   },
   "source": [
    "# Ch15_RNN과 CNN을 사용해 시퀀스 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T13:20:17.172676Z",
     "start_time": "2021-03-31T13:20:17.157701Z"
    },
    "id": "vb99gy5C_ePr"
   },
   "source": [
    "## 순환 신경망(Recurrent Neural Network, RNN) 요약\n",
    "- `Hands-on Machine Learning`의 내용 중에서 중요하다고 생각되는 내용이나 새롭게 알게 된\n",
    "내용을 간단하게 요약해서 정리하고자 한다.\n",
    "- 시간이 충분하지는 않아서 그림과 같은 자세한 설명은 생략하려고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGyv967w_ePs"
   },
   "source": [
    "## 15.1 순환 뉴런과 순환 층\n",
    "- RNN은 매 타임 스텝 t마다 모든 뉴런은 입력 벡터 $x_{(t)}$와 타임 스텝의 출력 벡터 $y_{(t-1)}$을 받는다.\n",
    "- 각 순환 뉴런은 입력 벡터 $x_{(t)}$를 위한 가중치 벡터 $w_x$와 출력 벡터 $y_{(t-1)}$를 위한 가중치 벡터 $w_y$를 가진다.\n",
    "- 이를 순환 층 전체로 생각하면 각각 가중치 행렬 $W_x$, $W_y$가 된다.\n",
    "- 순환 층 전체의 출력 벡터는 아래와 같은 식으로 표현된다.\n",
    "- 여기서 $\\mathbf{b}$는 편향이고 $\\phi$는 활성 함수이다.\n",
    "> $\\mathbf{y}_{(t)}=\\phi(\\mathbf{W}_{x}^{T}\\mathbf{x}_{(t)}+\\mathbf{W}_{x}^{T}\\mathbf{y}_{(t-1)}+\\mathbf{b})$\n",
    "- 식을 보면, $\\mathbf{y}_{(t)}$는 이전 상태의 출력인 $\\mathbf{y}_{(t-1)}$도 입력 받기 때문에, 결국 가장 첫 번째 입력인 $\\mathbf{x}_{0}$에 대한 값까지도 가지고 있게 된다.\n",
    "- 첫번째 타임 스텝 t=0에서는 이전 출력이 없으므로 모두 0이라고 가정한다.\n",
    "\n",
    "### 15.1.1 메모리 셀\n",
    "- 타입 스텝 t에서 순환 뉴런의 출력은 이전 타임 스텝의 모든 입력에 대한 값이므로 일종의 메모리 형태이다.\n",
    "- 타임 스텝에 걸쳐서 어떤 상태를 보존하는 신경망의 구성 요소를 메모리 셀(memory cell, 또는 셀)이라고 한다.\n",
    "- 타임 스텝 t에서의 셀의 상태 $\\mathbf{h}_{(t)}$(hidden cell)는 그 타임 스텝의 입력과 이전 타임 스텝의 상태에 대한 함수 $\\mathbf{h}_{(t)}=f(\\mathbf{h}_{(t-1)}, \\mathbf{x}_{(t)})$로 나타낼 수 있다.\n",
    "\n",
    "### 15.1.2 입력과 출력 시퀀스\n",
    "- Sequence-to-sequence network: 입력 시퀀스를 받아 출력 시퀀스를 만드는 RNN으로, 시계열 데이터를 예측하는데 유용하다.\n",
    "- Sequence-to-vector network: 입력 시퀀스를 받아 마지막 출력 벡터만 만드는 RNN이다.\n",
    "- Vector-to-sequence network: 각 타임 스텝에서 하나의 입력 벡터를 반복해서 입력하고 하나의 시퀀스를 출력하는 RNN으로,\n",
    "    예를 들어 이미지를 입력하여 이미지에 대한 캡션을 출력하는 RNN이 있다.\n",
    "- encoder-decoder: 인코더라 부르는 Sequence-to-vector network 뒤에 디코더라 부르는 Vector-to-sequence network를 연결하는 구조로, 문장 번역에 자주 사용된다.\n",
    "    - 한 언어의 문장을 네트워크에 입력하면, 인코더는 이 문장을 하나의 벡터로 변환하고, 디코더는 이 벡터를 받아 다른 언어의 문장으로 디코딩한다.\n",
    "    - 문장의 마지막 단어가 번역 시 첫번째 단어에 영향을 주는 경우가 많기 때문에, 인코더-디코더와 같은 이중 RNN은 하나의 Sequence-to-vector network보다 훨씬 더 잘 작동한다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK0ydQZh_ePu"
   },
   "source": [
    "## 15.2 RNN 훈련하기\n",
    "- RNN을 학습시킬 때에는 BPTT(Backpropagation Through Time)을 통해 학습하므로, 그레디언트가 마지막 출력뿐만 아니라 손실 함수를 사용한 모든 출력에 대해 역전파된다.\n",
    "- 또한 각 타임 스텝마다 같은 매개변수 $\\mathbf{W}$와 $\\mathbf{b}$가 사용되기 때문에 순전파에서 모두 동일한 가중치가 적용되어 계산이 진행된 후 역전파가 진행되면 모든 타임 스텝에 걸쳐 합산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T14:24:50.716431Z",
     "start_time": "2021-03-31T14:24:50.712431Z"
    },
    "id": "dPyQhIEf_ePw"
   },
   "source": [
    "## 15.3 시계열 예측하기\n",
    "- 시계열(Time series) 데이터: 타임 스텝마다 하나 이상의 값을 가지는 시퀀스 데이터.\n",
    "    - 단변량 시계열(Univariate time series): 타임 스텝마다 하나의 값을 가지는 데이터.\n",
    "    - 다변량 시계열(Multivariate time series): 타임 스텝마다 여러 값을 가지는 데이터.\n",
    "- 시계열을 다룰 때 입력 특성은 일반적으로 [배치 크기, 타임 스텝 수, 차원 수] 크기의 3D 배열로 나타낸다고 한다.\n",
    "- 따라서 단변량 시계열의 경우 차원 수는 1이 되고, 다변량 시계열의 경우 차원 수가 1 이상이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T03:47:00.105352Z",
     "start_time": "2021-04-01T03:47:00.095352Z"
    },
    "executionInfo": {
     "elapsed": 1677,
     "status": "ok",
     "timestamp": 1617793319489,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "envjtQxQ_ePx"
   },
   "outputs": [],
   "source": [
    "# 시계열 생성해주는 함수이다.\n",
    "# batch_size만큼 n_steps 길이의 여러 시계열을 만든다.\n",
    "# [배치 크기, 타임 스텝 수, 1] 크기의 넘파이 배열을 리턴하므로 이 시계열 데이터는 단변량 데이터이다.\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # 사인 곡선 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + 사인 곡선 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # + 잡음\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T08:14:40.713475Z",
     "start_time": "2021-04-01T08:14:40.666476Z"
    },
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1617793328116,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "VlBbeg5e_ePy"
   },
   "outputs": [],
   "source": [
    "# 위의 함수를 이용하여 훈련셋, 검증셋, 테스트셋을 생성한다.\n",
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26PYuIOF_ePz"
   },
   "source": [
    "### 15.3.1 기준 성능\n",
    "- RNN과 비교할 비교군으로 사용할 모델들을 생성한다.\n",
    "1. 순진한 예측(Naive forecasting): 각 시계열의 마지막 값을 그대로 예측\n",
    "    - 이렇게 하는 것도 높은 확률이 될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T15:53:47.201487Z",
     "start_time": "2021-03-31T15:53:47.186486Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1223,
     "status": "ok",
     "timestamp": 1617793333538,
     "user": {
      "displayName": "‍김태산(대학원생-자동차공학전공)",
      "photoUrl": "",
      "userId": "10967533091290920657"
     },
     "user_tz": -540
    },
    "id": "eq88g0US_eP1",
    "outputId": "33ec177d-686b-417c-8192-875b64e530e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse: 0.02091115154325962\n"
     ]
    }
   ],
   "source": [
    "y_pred = X_valid[:, -1]\n",
    "mse = np.mean(tf.keras.losses.mean_squared_error(y_valid, y_pred))\n",
    "print(f\"mse: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T15:16:48.646731Z",
     "start_time": "2021-03-31T15:16:48.637732Z"
    },
    "id": "fG1mrR9r_eP4"
   },
   "source": [
    "2. MLP(완전 연결층) 사용\n",
    "    - 검증셋에 대한 mse를 확인해보면, 순진한 예측보다는 좋은 결과를 출력하는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:48:13.892370Z",
     "start_time": "2021-04-01T05:48:07.363365Z"
    },
    "id": "ekIQ6rUI_eP5",
    "outputId": "6a9bf910-5692-4684-c155-3d00eddc45f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/1 - 0s - loss: 0.0042\n",
      "MSE of MLP: 0.004281704217195511\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[50, 1]),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, verbose=False)\n",
    "mse = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "print(f\"MSE of MLP: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T03:51:13.379038Z",
     "start_time": "2021-04-01T03:51:13.373038Z"
    },
    "id": "hc01qVyd_eP7"
   },
   "source": [
    "### 15.3.2 간단한 RNN 구현하기\n",
    "- `SimpleRNN()`은 하나의 뉴런으로 이루어진 하나의 층을 가지는 RNN 구조이다.\n",
    "- `SimpleRNN()`은 기본적으로 `tanh`를 활성 함수로 사용한다.\n",
    "- RNN은 어떤 길이의 타임 스텝도 처리할 수 있기 때문에 입력 시퀀스의 길이를 지정할 필요가 없다.\n",
    "- __return_sequences=True__로 옵션을 줄 경우 모든 타임 스텝마다 출력을 반환해준다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:49:53.200820Z",
     "start_time": "2021-04-01T05:48:35.683801Z"
    },
    "code_folding": [],
    "id": "EAu1XyeX_eP8",
    "outputId": "fa88b315-bde4-42e5-b287-57a6b7c5b06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/1 - 0s - loss: 0.0126\n",
      "MSE of SimpleRNN: 0.011292242005467416\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(units=1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(), loss=keras.losses.MeanSquaredError())\n",
    "model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "rnn_mse = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "print(f\"MSE of SimpleRNN: {rnn_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:56:13.807697Z",
     "start_time": "2021-04-01T05:56:13.797697Z"
    },
    "id": "ui6njx80_eP-"
   },
   "source": [
    "- 위의 결과를 보면, `SimpleRNN`이 `MLP`보다 좋지 못한 성능을 보여준다는 것을 알 수 있다.\n",
    "- 이는 `SimpleRNN`의 구조가 너무 간단하여 데이터에 과소적합되기 때문이다.\n",
    "- 생각해보면, `MLP`의 경우 모든 특징 벡터에 편향을 더한 수인 총 51개의 파라미터를 가지지만, `SimpleRNN`의 경우 매 타임 스텝마다의 입력과 곱하는 가중치 1개, 이전 상태의 값과 곱하는 가중치 1개에 편향을 더해 총 3개의 파라미터만을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T06:42:55.422309Z",
     "start_time": "2021-04-01T06:42:55.410308Z"
    },
    "id": "v1vK1PTy_eQB"
   },
   "source": [
    "### 15.3.3 심층 RNN\n",
    "- RNN은 기존의 인공신경망과 같이 셀을 여러 층으로 쌓는 것이 일반적이고, 이를 심층 RNN(deep RNN)이라고 한다.\n",
    "- `return_sequences=True`로 지정하지 않을 경우 (모든 타임 스텝에 대한 출력을 담은) 3D 배열이 아닌 \n",
    "(마지막 타임 스텝의 출력만 담은) 2D 배열이 리턴되므로 다음 `SimpleRNN`의 입력 형태와 맞지 않게 된다.\n",
    "- 따라서 모든 `SimpleRNN` 층에서 `return_sequences=True`로 지정해주어야 한다(마지막 출력만 관심 대상일 경우에는 \n",
    "아래와 같이 마지막 층에서는 설정하지 않으면 된다.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T06:49:13.846554Z",
     "start_time": "2021-04-01T06:48:12.724274Z"
    },
    "id": "kegKBOPI_eQC",
    "outputId": "9fa4e03d-7fdb-44f3-caf7-12f92ec1c444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 5s 770us/sample - loss: 0.0306\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 3s 411us/sample - loss: 0.0068\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 3s 421us/sample - loss: 0.0046\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 3s 407us/sample - loss: 0.0039\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 3s 411us/sample - loss: 0.0035\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 3s 406us/sample - loss: 0.0032\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 3s 409us/sample - loss: 0.0031\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 3s 410us/sample - loss: 0.0030\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 3s 406us/sample - loss: 0.0029\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 3s 410us/sample - loss: 0.0029\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 3s 412us/sample - loss: 0.0028\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 3s 409us/sample - loss: 0.0029\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 3s 408us/sample - loss: 0.0028\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 3s 408us/sample - loss: 0.0028\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 3s 409us/sample - loss: 0.0027\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 3s 405us/sample - loss: 0.0028\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 3s 410us/sample - loss: 0.0027\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 3s 406us/sample - loss: 0.0027\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 3s 409us/sample - loss: 0.0027\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 3s 409us/sample - loss: 0.0027\n",
      "2000/1 - 1s - loss: 0.0030\n",
      "MSE of deep RNN: 0.002827187195420265\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    SimpleRNN(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=1)\n",
    "drnn_mse = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "print(f\"MSE of deep RNN: {drnn_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T06:35:58.740861Z",
     "start_time": "2021-04-01T06:35:58.730860Z"
    },
    "id": "s0sjhopS_eQD"
   },
   "source": [
    "- 출력 결과를 보면, `deep RNN`이 `MLP`보다 성능이 더 좋은 것을 확인할 수 있다.\n",
    "- 그러나 위의 모델의 경우 마지막 층까지 RNN을 사용하였는데, 마지막 층의 은닉 상태는 크게 필요하지 않는다고 한다.\n",
    "- 이는 RNN이 한 타임 스텝에서 다음 타임 스텝으로 필요한 정보를 나를 때 마지막 층이 아닌 다른 층의 은닉 상태를 주로 사용할\n",
    "것이기 때문이라고 한다.\n",
    "- 아마 마지막 층에 해당하는 은닉 상태의 경우 단지 출력값을 내는데에만 사용되기 때문이라는 말인 것 같다.\n",
    "- 또한 `SimpleRNN`의 경우 `tanh`를 활성 함수로 사용하기 때문에 예측값이 -1 ~ 1 사이에 놓이게 되므로 이 또한 유용하지 않다,\n",
    "- 이러한 이유로 출력층의 경우 `Dence` 층으로 사용하는 경우가 많다고 한다.\n",
    "- 이에 대한 코드는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T07:02:50.283538Z",
     "start_time": "2021-04-01T07:02:08.716432Z"
    },
    "id": "vFfuY6Yf_eQE",
    "outputId": "9fec46a6-1a47-44d8-c0e3-934eb86986e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 - 4s - loss: 0.0756\n",
      "Epoch 2/20\n",
      "7000/7000 - 2s - loss: 0.0143\n",
      "Epoch 3/20\n",
      "7000/7000 - 2s - loss: 0.0075\n",
      "Epoch 4/20\n",
      "7000/7000 - 2s - loss: 0.0052\n",
      "Epoch 5/20\n",
      "7000/7000 - 2s - loss: 0.0043\n",
      "Epoch 6/20\n",
      "7000/7000 - 2s - loss: 0.0037\n",
      "Epoch 7/20\n",
      "7000/7000 - 2s - loss: 0.0034\n",
      "Epoch 8/20\n",
      "7000/7000 - 2s - loss: 0.0033\n",
      "Epoch 9/20\n",
      "7000/7000 - 2s - loss: 0.0031\n",
      "Epoch 10/20\n",
      "7000/7000 - 2s - loss: 0.0030\n",
      "Epoch 11/20\n",
      "7000/7000 - 2s - loss: 0.0030\n",
      "Epoch 12/20\n",
      "7000/7000 - 2s - loss: 0.0030\n",
      "Epoch 13/20\n",
      "7000/7000 - 2s - loss: 0.0029\n",
      "Epoch 14/20\n",
      "7000/7000 - 2s - loss: 0.0029\n",
      "Epoch 15/20\n",
      "7000/7000 - 2s - loss: 0.0028\n",
      "Epoch 16/20\n",
      "7000/7000 - 2s - loss: 0.0028\n",
      "Epoch 17/20\n",
      "7000/7000 - 2s - loss: 0.0028\n",
      "Epoch 18/20\n",
      "7000/7000 - 2s - loss: 0.0028\n",
      "Epoch 19/20\n",
      "7000/7000 - 2s - loss: 0.0028\n",
      "Epoch 20/20\n",
      "7000/7000 - 2s - loss: 0.0028\n",
      "2000/1 - 1s - loss: 0.0030\n",
      "MSE of deep RNN: 0.0027124519646167756\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    SimpleRNN(20),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
    "denseRNN_mse = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "print(f\"MSE of deep RNN: {denseRNN_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uAKBs_-_eQF"
   },
   "source": [
    "- 위와 같이 마지막 층을 `Dense` 층으로 바꿀 경우 훈련 시간은 줄어듦과 동시에 비슷한 정확도를 낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:31:50.581602Z",
     "start_time": "2021-04-04T05:31:50.571601Z"
    },
    "id": "_eKcDhga_eQF"
   },
   "source": [
    "### 15.3.4 여러 타임 스텝 앞을 예측하기\n",
    "- RNN을 훈련하여 다음 값 10개를 한 번에 예측할 수도 있다.\n",
    "- 이 경우 Sequence-to-vector network를 사용하면 된다.\n",
    "- 또한 데이터셋도 타깃값의 갯수가 10개가 되도록 바꾸어주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T08:15:01.150859Z",
     "start_time": "2021-04-01T08:15:01.105860Z"
    },
    "id": "6MQ6osjR_eQG"
   },
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T08:23:47.926583Z",
     "start_time": "2021-04-01T08:21:15.262569Z"
    },
    "id": "jw1NsCjk_eQH",
    "outputId": "60d63634-163a-4718-b7b7-6832a67fde2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0530\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0217\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0167\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0137\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0120\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0113\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0106\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0101\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0100\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0098\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0094\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0093\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0091\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0090\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0090\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0089\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0087\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0088\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0087\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0087\n",
      "2000/1 - 1s - loss: 0.0073\n",
      "mse of 10 outputs: 0.00827310950309038\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    SimpleRNN(20),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='Adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=20, verbose=1)\n",
    "vec_mse = model.evaluate(X_valid, y_valid, verbose=2)\n",
    "print(f\"mse of 10 outputs: {vec_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQDpae8u_eQI"
   },
   "source": [
    "- 출력 결과를 보면, 다음 10개 타임 스텝에 대한 MSE는 약 0.008로, 추가하지는 않았지만 책에서 기술한 MLP의 MSE 0.0188보다\n",
    "좋은 성능을 보임을 알 수 있다.\n",
    "- 위의 sequence-to-vector network는 마지막 타임 스텝에서만 다음 값 10개의 벡터를 예측값으로 출력한다.\n",
    "- 이 대신 모든 타임 스텝에서 다음 값 10개를 예측하도록 모델을 sequence-to-sequence network로 만들 수도 있다.\n",
    "- 즉, 타임 스텝 0에서 모델이 타임 스텝 1에서 10까지의 예측값을 출력하고, 타임 스텝 1에서는 타임 스텝 2에서 11까지의\n",
    "예측값을 출력하는 식으로 계속한다는 의미이다.\n",
    "- sequence-to-sequence network로 모델을 생성할 경우 마지막 타임 스텝에서의 출력뿐만 아니라 모든 타임 스텝에서\n",
    "RNN 출력에 대한 항이 loss에 포함되므로 더 많은 loss의 그레디언트가 모델로 흐르게 되고, 또한 각 타임 스텝의 출력에서도\n",
    "그레디언트가 흐를 수 있다.\n",
    "- 이를 통해 훈련을 더 안정적으로 만들고 훈련 속도도 높아진다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T14:49:14.537462Z",
     "start_time": "2021-04-01T14:49:14.530461Z"
    },
    "id": "aVQEnile_eQI"
   },
   "source": [
    "- sequence-to-sequence network를 적용하기 위해서는 타깃의 형태도 이에 맞게 아래와 같이 수정해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T14:39:57.615619Z",
     "start_time": "2021-04-01T14:39:57.555620Z"
    },
    "id": "ZjJi8igN_eQJ"
   },
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10)) # 각 타깃은 10D 벡터의 시퀀스이다.\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mHcNVKJ_eQK"
   },
   "source": [
    "- sequence-to-sequence network 모델을 사용하려면 모든 RNN 층을 return_sequences=True로 설정해주어야 한다(매 타임 스텝마다 출력을 내야 하므로).\n",
    "- 그다음 모든 타임 스텝에서 출력을 Dense 층에 적용해야 하는데, 이 때 케라스의 TimeDistributed 층을 사용한다.\n",
    "- TimeDistributed 층은 다른 층(예제에서는 Dense 층)을 감싸서 입력 시퀀스의 모든 타임 스텝에 해당 층을 적용한다.\n",
    "- 각 타임 스텝을 별개의 샘플처럼 다루도록 Dense 층에 입력하기 전에 입력의 크기를 바꿔주고(Dense 층의 경우 1D 입력을 받으므로) Dense 층을 적용한다.\n",
    "- Dense 층을 적용한 후에는 출력 크기를 다시 시퀀스로 되돌린다.\n",
    "- 예시를 통한 설명은 p.612를 확인하는 것이 좋다.\n",
    "- 훈련하는 동안에는 모든 출력이 필요하지만, 예측과 평가에는 마지막 타임 스텝의 출력만 필요하다.\n",
    "- 따라서 평가를 위해서 마지막 타임 스텝의 출력에 대한 MSE만을 계산하는 함수를 작성하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T15:18:46.274043Z",
     "start_time": "2021-04-01T15:16:13.546963Z"
    },
    "id": "9eVWFZS9_eQL",
    "outputId": "88b8bb0e-d0ce-45a4-ba7b-e2cbf7973796",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples\n",
      "Epoch 1/20\n",
      "7000/7000 [==============================] - 9s 1ms/sample - loss: 0.0483 - last_time_step_mse: 0.0370\n",
      "Epoch 2/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0347 - last_time_step_mse: 0.0224\n",
      "Epoch 3/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0317 - last_time_step_mse: 0.0205\n",
      "Epoch 4/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0284 - last_time_step_mse: 0.0162\n",
      "Epoch 5/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0258 - last_time_step_mse: 0.0134\n",
      "Epoch 6/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0241 - last_time_step_mse: 0.0119\n",
      "Epoch 7/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0225 - last_time_step_mse: 0.0102\n",
      "Epoch 8/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0215 - last_time_step_mse: 0.0092\n",
      "Epoch 9/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0206 - last_time_step_mse: 0.0084\n",
      "Epoch 10/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0203 - last_time_step_mse: 0.0083\n",
      "Epoch 11/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0198 - last_time_step_mse: 0.0079\n",
      "Epoch 12/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0194 - last_time_step_mse: 0.0074\n",
      "Epoch 13/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0191 - last_time_step_mse: 0.0072\n",
      "Epoch 14/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0191 - last_time_step_mse: 0.0075\n",
      "Epoch 15/20\n",
      "7000/7000 [==============================] - 8s 1ms/sample - loss: 0.0190 - last_time_step_mse: 0.0074\n",
      "Epoch 16/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0184 - last_time_step_mse: 0.0066\n",
      "Epoch 17/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0187 - last_time_step_mse: 0.0071\n",
      "Epoch 18/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0185 - last_time_step_mse: 0.0070\n",
      "Epoch 19/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0185 - last_time_step_mse: 0.0069\n",
      "Epoch 20/20\n",
      "7000/7000 [==============================] - 7s 1ms/sample - loss: 0.0185 - last_time_step_mse: 0.0070\n",
      "2000/1 - 1s - loss: 0.0172 - last_time_step_mse: 0.0062\n",
      "last_mse: 0.006194211542606354\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(Dense(10))\n",
    "])\n",
    "\n",
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=[last_time_step_mse])\n",
    "model.fit(X_train, Y_train, epochs=20, verbose=1)\n",
    "mse, last_mse = model.evaluate(X_valid, Y_valid, verbose=2)\n",
    "print(f\"last_mse: {last_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T05:32:54.841559Z",
     "start_time": "2021-04-04T05:32:54.808456Z"
    },
    "id": "R-rMR80C_eQN"
   },
   "source": [
    "- 검증셋을 통해 MSE를 확인한 결과 약 0.006으로 기존의 마지막 타임 스텝에서만 출력을 내던 모델보다 약 25% 향상된 성능을\n",
    "보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T15:30:54.085329Z",
     "start_time": "2021-04-01T15:30:53.791031Z"
    },
    "id": "O_qBz6fw_eQO"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJQoOhan_eQO"
   },
   "source": [
    "## 15.4 긴 시퀀스 다루기\n",
    "- 긴 시퀀스의 데이터로 RNN을 훈련시킬 때에는 많은 타임 스텝에 걸쳐 RNN이 실행되므로 펼친 RNN이 매우 깊은 네트워크가 된다.\n",
    "- 이러한 경우 훈련하는 데 매우 긴 시간이 걸리거나 그레디언트 소실 및 폭주 문제와 같이 훈련이 불안정할 수 있다.\n",
    "- 또한 RNN이 긴 시퀀스를 처리할 때 입력의 첫 부분을 조금씩 잊어버리게 된다.\n",
    "### 15.4.1 불안정한 그레디언트 문제와 싸우기\n",
    "- 불안정한 그레디언트 문제를 완화하기 위해서 기존의 심층 신경망에 사용하던 규제(가중치 초기화, 옵티마이저, 드롭아웃)들을 \n",
    "RNN에도 적용할 수 있다.\n",
    "- 활성 함수로 `ReLU`를 사용하는 것은 출력이나 그레디언트가 폭주할 수 있으므로 RNN에서는 `ReLU` 대신 수렴하는 활성 함수인 \n",
    "`tanh`를 주로 사용한다.\n",
    "- 배치 정규화(Batch nomalization)의 경우 타임 스텝 사이에 적용하는 것은 좋은 결과를 내지 못하고, 순환 층 사이에 적용했을\n",
    "때 없는 것보다 조금 나은 결과를 낸다고 한다.\n",
    "- 케라스에서는 각 순환 층 이전에 BatchNormalization  층을 추가하면 된다.\n",
    "---\n",
    "- RNN에서 잘 맞는 정규화는 층 정규화(Layer normalization)이다.\n",
    "- 층 정규화는 배치 정규화와 비슷하지만, 배치 차원에 대해 정규화하는 대신 특성 차원에 대해 정규화한다.\n",
    "- 이를 통해 각각의 샘플에 대해 독립적으로 매 타임 스텝마다 동적으로 필요한 통계를 계산한다.\n",
    "- 배치 정규화와 마찬가지로 층 정규화는 입력마다 하나의 스케일과 이동 파라미터를 학습하고, 일반적으로 입력과 은닉 상태의 선형 조합 직후에 사용한다.\n",
    "- 아래는 tf.keras를 사용하여 간단한 RNN과 층 정규화를 구현한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T06:45:01.623252Z",
     "start_time": "2021-04-04T06:45:01.581282Z"
    },
    "id": "lzndU9jC_eQP"
   },
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation='tanh', **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # RNN에서는 은닉 상태의 개수와 출력의 개수가 동일하다.\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        # 활성 함수 전에 층 정규화를 하기 위해 RNNCell을 선언할 때 활성 함수를 입력하지 않는다.\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "                                                \n",
    "    def call(self, inputs, states):\n",
    "        # call 메서드에서 new_states가 따로 사용되지 않는 이유는SimpleRNNCell에서의 출력 outputs와 new_states는 \n",
    "        # 동일하기 때문이다. 즉, outputs == new_states[0]\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        # call 메서드는 두 개의 출력을 리턴하는데, 매 타임 스텝에서 하나는 출력이고 다른 하나는 새로운 은닉 상태이다.                                  \n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HCxlUAM_eQR"
   },
   "source": [
    "- 위의 사용자 정의 셀을 사용할 때에는 아래와 같이 `keras.layers.RNN` 층을 만들어 셀의 객체를 입력하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T06:45:12.430124Z",
     "start_time": "2021-04-04T06:45:03.616304Z"
    },
    "id": "FP9pvImP_eQS"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ_DQ-w7_eQS"
   },
   "source": [
    "- 비슷하게 타임 스텝 사이에 드롭아웃을 적용하는 사용자 정의 셀도 만들 수 있다.\n",
    "- 그러나 keras에서 제공하는 모든 순환 층과 셀은 dropout 매개변수와 reccurent_dropout 매개변수를 지원하므로, 이를 사용하는\n",
    "것이 더 편리하다.\n",
    "- dropout 매개변수는 타임 스텝마다 입력에 적용할 드롭아웃 비율을 정의하고, reccurent_dropout 매개변수는 타임 스텝마다 은닉\n",
    "상태에 대한 드롭아웃 비율을 정의한다.\n",
    "- 위와 같은 여러 가지 기법들로 불안정한 그레디언트 문제를 줄이고 RNN을 효율적으로 훈련할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZx3asSv_eQT"
   },
   "source": [
    "### 15.4.2 단기 기억 문제 해결하기\n",
    "- 그러나 RNN을 학습시킬 데이터의 길이가 긴 경우(긴 문장과 같이) 어느 정도 시간이 지나면 RNN의 상태는 사실상 첫 번째 입력의 흔적을 전혀 가지고 있지 않는다.\n",
    "- 이런 문제를 해결하기 위해 장기 메모리를 가지는 여러 종류의 셀이 연구되었고, `LSTM`은 이중에서 현재 가장 인기 있는 장기 메모리를 가진 셀이다.\n",
    "- 요즘에는 LSTM과 같은 셀들의 성능이 매우 좋아 기본 RNN 셀은 거의 사용되지 않는다.\n",
    "---\n",
    "__LSTM 셀__\n",
    "- LSTM(Long Short-Term Memory, 장단기 메모리) 셀은 기본 RNN 셀과 비슷하게 사용되지만, 훈련이 빠르게 수렴하고 데이터에 있는 장기간의 의존성을 감지하여 성능은 훨씬 좋다.\n",
    "- keras에서는 간단하게 `SimpleRNN` 층 대신 `LSTM` 층을 사용하면 된다.\n",
    "- 또는 아래와 같이 keras.layers.RNN 층에 `LSTMCell`을 매개변수로 지정할 수도 있지만, LSTM 층이 GPU 상에서 실행할 때 최적화\n",
    "된 구현을 사용하기 때문에 일반적으로 LSTM 층을 사용하는 것이 선호된다고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T07:01:35.246209Z",
     "start_time": "2021-04-04T07:01:30.842894Z"
    },
    "id": "v2Nf-Rhy_eQV"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T07:07:15.419123Z",
     "start_time": "2021-04-04T07:07:13.347824Z"
    },
    "id": "on0o4Lqo_eQV"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCknzVv__eQW"
   },
   "outputs": [],
   "source": [
    "- LSTM 층의 구조는 아래와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-04T07:38:22.999856Z",
     "start_time": "2021-04-04T07:38:22.839787Z"
    },
    "id": "g0zfIgaD_eQX"
   },
   "source": [
    "![RESULT](./LSTM_Cell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvDWlwIW_eQY"
   },
   "source": [
    "- 위의 그림에서 $\\mathbf{h}_{(t)}$를 단기 상태(short-term state), $\\mathbf{c}_{(t)}$를 장기 상태(long-term state)라고 \n",
    "한다.\n",
    "- 장기 상태 $\\mathbf{c}_{(t-1)}$은 네트워크를 왼쪽에서 오른쪽으로 거치면서, 삭제 게이트(forget gate)를 지나 일부 메모리를\n",
    "잃고, 입력 게이트(input gate)를 통과한 메모리를 덧셈 연산\n",
    "을 통해 추가한다.\n",
    "- 이를 통해 새로운 장기 상태인 $\\mathbf{c}_{(t)}$가 만들어지고 다른 추가 변환 없이 바로 출력으로 보내진다.\n",
    "- 따라서 위 과정에서 타임 스텝마다 일부 메모리(중요도가 낮은 메모리)는 삭제되고 일부 메모리(중요도가 높은 메모리)은 \n",
    "추가된다.\n",
    "- 장기 상태 $\\mathbf{c}_{(t)}$는 복사되어 tanh 함수를 거친 후 출력 게이트(output gate)에 의해 필터링되어 단기 상태\n",
    "$\\mathbf{h}_{(t)}$를 만들어낸다.\n",
    "- 단기 상태 $\\mathbf{h}_{(t)}$는 타임 스텝에서 셀의 출력 $\\mathbf{y}_{(t)}$와 동일하다.\n",
    "--- \n",
    "- 각각의 게이트의 역할은 아래와 같다.\n",
    "- 현재 입력 벡터 $\\mathbf{x}_{(t)}$와 이전의 단기 상태 $\\mathbf{h}_{(t-1)}$이 4개의 서로 다른 FC 층에 입력된다.\n",
    "- 먼저 주 층은 $\\mathbf{g}_{(t)}$를 출력하는 층으로, 현재 입력 $\\mathbf{x}_{(t)}$와 이전의 단기 상태 $\\mathbf{h}_{(t-1)}$을 분석하는 일반적인 역할을 한다.\n",
    "- LSTM에서는 이 층의 출력이 곧바로 나가는 것이 아니라, 장기 상태에 가장 중요한 부분만 저장된다.\n",
    "- 나머지 3개의 층은 게이트 제어기(gate controller)라고 불리고, 활성 함수로 로지스틱 시그모이드가 사용되기 때문에 출력의 범위가 0에서 1 사이(확률값)가 되어 출력을 제어한다.\n",
    "- 의미를 생각해보면, 0이면 게이트를 닫고 1이면 게이트를 여는 셈이다.\n",
    "- 삭제 게이트($\\mathbf{f}_{(t)}$에 의해 제어)는 장기 상태의 어느 부분이 삭제되어야 하는지를 제어한다.\n",
    "- 입력 게이트($\\mathbf{i}_{(t)}$에 의해 제어)는 $\\mathbf{g}_{(t)}$의 어느 부분이 장기 상태에 더해져야 하는지를 제어한다.\n",
    "- 출력 게이트($\\mathbf{o}_{(t)}$에 의해 제어)는 장기 상태의 어느 부분을 읽어서 현재 타임 스텝의 $\\mathbf{h}_{(t)}$와 $\\mathbf{y}_{(t)}$로 출력해야 할지를 제어한다.\n",
    "- 이를 통해 LSTM 셀은 시계열, 긴 텍스트, 오디오 녹음 등의 장기 데이터로부터 장기 패턴을 매우 훌륭하게 잡아낸다.\n",
    "---\n",
    "- LSTM에서 하나의 샘플에 대해 타임 스텝마다 각 게이트 출력과 셀의 장기 상태와 단기 상태 및 출력에 대한 수식은 아래와 같다.\n",
    "  - $\\mathbf{i}_{(t)}=\\sigma({\\mathbf{W}_{xi}}^{T}\\mathbf{x}_{(t)}+{\\mathbf{W}_{hi}}^{T}\\mathbf{h}_{(t-1)}+\\mathbf{b}_{i})$\n",
    "  - $\\mathbf{f}_{(t)}=\\sigma({\\mathbf{W}_{xf}}^{T}\\mathbf{x}_{(t)}+{\\mathbf{W}_{hf}}^{T}\\mathbf{h}_{(t-1)}+\\mathbf{b}_{f})$\n",
    "  - $\\mathbf{o}_{(t)}=\\sigma({\\mathbf{W}_{xo}}^{T}\\mathbf{x}_{(t)}+{\\mathbf{W}_{ho}}^{T}\\mathbf{h}_{(t-1)}+\\mathbf{b}_{o})$\n",
    "  - $\\mathbf{g}_{(t)}=tanh({\\mathbf{W}_{xg}}^{T}\\mathbf{x}_{(t)}+{\\mathbf{W}_{hg}}^{T}\\mathbf{h}_{(t-1)}+\\mathbf{b}_{g})$\n",
    "  - $\\mathbf{c}_{(t)}=\\mathbf{f}_{(t)}\\bigotimes\\mathbf{c}_{(t-1)}+\\mathbf{i}_{(t)}\\bigotimes\\mathbf{g}_{(t)}$\n",
    "  - $\\mathbf{y}_{(t)}=\\mathbf{h}_{(t)}=\\mathbf{o}_{(t)}\\bigotimes tanh(\\mathbf{c}_{(t)})$\n",
    "- $\\mathbf{W}_{x}$는 입력 벡터 $\\mathbf{x}_{(t)}$와 연결된 FC층의 가중치 행렬이다.\n",
    "- $\\mathbf{W}_{h}$는 이전의 단기 상태 $\\mathbf{h}_{(t-1)}$와 연결된 FC층의 가중치 행렬이다.\n",
    "- $\\mathbf{b}$는 각각의 FC층에서의 편향이다.\n",
    "---\n",
    "__핍홀 연결__\n",
    "- 핍홀 연결(peephole connection)은 LSTM의 변종으로, 게이트 제어기에 입력 $\\mathbf{x}_{(t)}$와 이전 단기 상태 $\\mathbf{h}_{(t-1)}$ 뿐만이 아니라 장기 상태도 입력하여 좀 더 많은 문맥을 감지하도록 한다.\n",
    "- 이를 통해 성능이 향상되는 경우가 많지만, 항상 그런 것은 아니고 핍홀의 여부에 따라 어떤 종류의 작업이 향상되는지도 명확하지 않으므로 사용할 때 직접 확인해야 한다고 한다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ch15_RNN과 CNN을 사용해 시퀀스 처리하기.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
